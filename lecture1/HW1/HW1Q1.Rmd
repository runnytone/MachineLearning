---
title: "Machine Learning"
subtitle: "HW1"
author: |
    | Xin Cheng
    | runnytone@uchicago.edu
date: "01/20/2018"
fontsize: 10pt
output: 
    pdf_document:
        fig_width: 6
        fig_height: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r,eval=FALSE}
#List the packages we need, install if missing, then load all of them
PackageList =c('MASS','data.table','tree','kknn','rpart','rpart.plot') 
NewPackages=PackageList[!(PackageList %in% 
                            installed.packages()[,"Package"])]
if(length(NewPackages)) install.packages(NewPackages)

lapply(PackageList,require,character.only=TRUE)#array function

download.file("https://raw.githubusercontent.com/ChicagoBoothML/HelpR/master/docv.R", "docv.R")
source("docv.R") #this has docvknn used below

set.seed(2018) #Always set the seed for reproducibility
```
# Q1
## Q1.1
```{r}
set.seed(2018)
x <- rnorm(100, mean=0, sd=1)
varepsilon <- rnorm(100, mean=0, sd=1)
y <- 1.8*x + 2 + varepsilon
train <- data.frame(y,x)
train <- train[order(train$x),]

x <- rnorm(10000, mean=0, sd=1)
varepsilon <- rnorm(10000, mean=0, sd=1)
y <- 1.8*x + 2 + varepsilon
test <- data.frame(y, x)
test <- test[order(test$x),]
rm(x,y)
```

## Q1.2
```{r}
plot(train$x, train$y, main = "a scatter plot of y vs x", xlab = "x", ylab = "y")+
lines(train$x, 1.8*train$x + 2, col="black", lwd = 2)
```
## Q1.3
```{r}
ls <- lm(train$y~train$x, train)
print(summary(ls))

plot(train$x, train$y, main = "a scatter plot of y vs x", xlab = "x", ylab = "y")+
lines(train$x, 1.8*train$x + 2, col="black", lwd = 2)+
abline(ls$coef, col="blue", lwd = 2, lty = "dashed")
```

## Q1.4
```{r}
library(kknn)
kvec <- c(2:15)
kknn.train <- list()
fitted.train <- data.frame(matrix(NA, nrow = 100, ncol = length(kvec)))

for (i in 1:length(kvec)) {
  kknn.train[[i]] <- kknn(y~x, train, train, k = kvec[i], kernel = "rectangular") 
  fitted.train[, i] <- kknn.train[[i]]$fitted 
  }

par(mfrow=c(1,2))

plot(train$x, train$y, main = "k=2", xlab = "x", ylab = "y")+
lines(train$x, 1.8*train$x + 2, col="black", lwd = 2)+
lines(train$x, fitted.train[, 1], col="blue", lwd = 2)

plot(train$x, train$y, main = "k=12", xlab = "x", ylab = "y")+
lines(train$x, 1.8*train$x + 2, col="black", lwd = 2)+
lines(train$x, fitted.train[, 11], col="blue", lwd = 2)
```

## Q1.5
```{r}
kvec <- c(2:15)
kknn.test <- list()
fitted.test <- data.frame(matrix(NA, nrow = 10000, ncol = length(kvec)))
mse <- as.numeric()

for(i in 1:length(kvec)) {
  kknn.test[[i]] <- kknn(y~x, train, test, k = kvec[i], kernel = "rectangular") 
  fitted.test[, i] <- kknn.test[[i]]$fitted 
  mse[i] = mean((test$y-fitted.test[,i])^2)
}
cat("the best k is: ", kvec[which.min(mse)])

mse.ls <- mean((test$y-ls$coefficients[1]-ls$coefficients[2]*test$x)^2)

plot(log(1/kvec), mse)+
abline(h = mse.ls, lwd=2, col = "lightgray", lty = 3)

cat("the smallest MSE of knn is", min(mse), ",while the MES of linear gression is", mse.ls, ". \n Linear gression model fits better in this case.")
```

### Note
kknn(y~x, train, train, k = k, kernel = "rectangular")$fitted
gives you prediction of y in train dataset

kknn(y~x, train, test, k = k, kernel = "rectangular")$fitted
gives you prediction of y in test dataset

The solution should consider naming the columns of the two variables the same and be agnostic to who you are calling in the formula attribute. Otherwise the program will consider that the only information that is reliable is the one in the formula and will only use it and predict a database with its own data.

## Q1.6
```{r, echo=FALSE}
set.seed(2018)
x <- rnorm(100, mean=0, sd=1)
varepsilon <- rnorm(100, mean=0, sd=1)
y <- tanh(1.1*x) + 2 + varepsilon
train <- data.frame(y,x)
train <- train[order(train$x),]

x <- rnorm(10000, mean=0, sd=1)
varepsilon <- rnorm(10000, mean=0, sd=1)
y <- tanh(1.1*x) + 2 + varepsilon
test <- data.frame(y, x)
test <- test[order(test$x),]
rm(x,y)

ls <- lm(train$y~train$x, train)
print(summary(ls))

plot(train$x, train$y, main = "a scatter plot of y vs x", xlab = "x", ylab = "y")+
lines(train$x, tanh(train$x*1.1) + 2, col="black", lwd = 2)+
abline(ls$coef, col="blue", lwd = 2, lty = "dashed")

kvec <- c(2:15)
kknn.train <- list()
fitted.train <- data.frame(matrix(NA, nrow = 100, ncol = length(kvec)))

for (i in 1:length(kvec)) {
  kknn.train[[i]] <- kknn(y~x, train, train, k = kvec[i], kernel = "rectangular") 
  fitted.train[, i] <- kknn.train[[i]]$fitted 
  }

par(mfrow=c(1,2))

plot(train$x, train$y, main = "k=2", xlab = "x", ylab = "y")+
lines(train$x, tanh(train$x*1.1) + 2, col="black", lwd = 2)+
lines(train$x, fitted.train[, 1], col="blue", lwd = 2)

plot(train$x, train$y, main = "k=12", xlab = "x", ylab = "y")+
lines(train$x, tanh(train$x*1.1) + 2, col="black", lwd = 2)+
lines(train$x, fitted.train[, 11], col="blue", lwd = 2)

kknn.test <- list()
fitted.test <- data.frame(matrix(NA, nrow = 10000, ncol = length(kvec)))
mse <- as.numeric()

for(i in 1:length(kvec)) {
  kknn.test[[i]] <- kknn(y~x, train, test, k = kvec[i], kernel = "rectangular") 
  fitted.test[, i] <- kknn.test[[i]]$fitted 
  mse[i] = mean((test$y-fitted.test[,i])^2)
}

cat("the best k is: ", kvec[which.min(mse)])

mse.ls <- mean((test$y-ls$coefficients[1]-ls$coefficients[2]*test$x)^2)

par(mfrow=c(1,1))
plot(log(1/kvec), mse)+
abline(h = mse.ls, lwd=2, col = "lightgray", lty = 3)

cat("the smallest MSE of knn is", min(mse), ",while the MES of linear gression is", mse.ls, ". \n linear gression model fits better in this case.")
```

## Q1.7
```{r, echo=FALSE}
set.seed(2018)
x <- rnorm(100, mean=0, sd=1)
varepsilon <- rnorm(100, mean=0, sd=1)
y <- sin(2*x) + 2 + varepsilon
train <- data.frame(y,x)
train <- train[order(train$x),]

x <- rnorm(10000, mean=0, sd=1)
varepsilon <- rnorm(10000, mean=0, sd=1)
y <- sin(2*x) + 2 + varepsilon
test <- data.frame(y, x)
test <- test[order(test$x),]
rm(x,y)

ls <- lm(train$y~train$x, train)
print(summary(ls))

plot(train$x, train$y, main = "a scatter plot of y vs x", xlab = "x", ylab = "y")+
lines(train$x, sin(train$x*2) + 2, col="black", lwd = 2)+
abline(ls$coef, col="blue", lwd = 2, lty = "dashed")

kvec <- c(2:15)
kknn.train <- list()
fitted.train <- data.frame(matrix(NA, nrow = 100, ncol = length(kvec)))

for (i in 1:length(kvec)) {
  kknn.train[[i]] <- kknn(y~x, train, train, k = kvec[i], kernel = "rectangular") 
  fitted.train[, i] <- kknn.train[[i]]$fitted 
  }

par(mfrow=c(1,2))

plot(train$x, train$y, main = "k=2", xlab = "x", ylab = "y")+
lines(train$x, sin(train$x*2) + 2, col="black", lwd = 2)+
lines(train$x, fitted.train[, 1], col="blue", lwd = 2)

plot(train$x, train$y, main = "k=12", xlab = "x", ylab = "y")+
lines(train$x, sin(train$x*2) + 2, col="black", lwd = 2)+
lines(train$x, fitted.train[, 11], col="blue", lwd = 2)

kknn.test <- list()
fitted.test <- data.frame(matrix(NA, nrow = 10000, ncol = length(kvec)))
mse <- as.numeric()

for(i in 1:length(kvec)) {
  kknn.test[[i]] <- kknn(y~x, train, test, k = kvec[i], kernel = "rectangular") 
  fitted.test[, i] <- kknn.test[[i]]$fitted 
  mse[i] = mean((test$y-fitted.test[,i])^2)
}

cat("the best k is: ", kvec[which.min(mse)])

mse.ls <- mean((test$y-ls$coefficients[1]-ls$coefficients[2]*test$x)^2)

par(mfrow=c(1,1))
plot(log(1/kvec), mse)+
abline(h = mse.ls, lwd=2, col = "lightgray", lty = 3)

cat("the smallest MSE of knn is", min(mse), ",while the MES of linear gression is", mse.ls, ".\n knn model fits better in this case.")
```

## Q1.8
```{r, echo=FALSE}
set.seed(2018)

x <- matrix(rnorm(100*20, mean=0, sd=1) ,100, 20)
varepsilon <- rnorm(100, mean=0, sd=1)
y <- sin(2*x[,1]) + 2 + varepsilon
train <- data.frame(y,x)
all.train <- train[order(train[,2]),]

x <- matrix(rnorm(10000*20, mean=0, sd=1) ,10000, 20)
varepsilon <- rnorm(10000, mean=0, sd=1)
y <- sin(2*x[,1]) + 2 + varepsilon
test <- data.frame(y, x)
all.test <- test[order(test[,2]),]
rm(x,y)



kvec <- c(2:15)
kknn.test <- list()
fitted.test <- data.frame(matrix(NA, nrow = 10000, ncol = length(kvec)))
mse.kknn <- data.frame(matrix(NA, nrow = length(kvec), ncol = 20))


for (j in 1:20) {
  train <- all.train[,1:(j+1)]
  test <- all.test[,1:(j+1)]
  for (i in 1:length(kvec)) {
  fitted.test <- kknn(y~., train, test, k = kvec[i], kernel = "rectangular")$fitted 
  mse.kknn[i,j] <- mean((test$y-fitted.test)^2)
  } 
  ls <- lm(train$y~., train)
  mse.ls[j] <- mean((test$y-ls$coefficients[1]-t(as.matrix(ls$coefficients[-1]))%*%t(as.matrix(test[, -1])))^2)
}

colors <- colorRampPalette(c("blue", "red"))(20)
for (j in 1:20) {
par(new=TRUE)
plot(log(1/kvec), mse.kknn[,j], ylim = c(1,2), type = "l", col = colors[j])+ 
abline(h = mse.ls[j], lwd=2, col = colors[j], lty = 3)
}
cat("as number of variables increase, mse for both models increase. \n on average, knn models with large k is better than linear model")
```

## Q1.9
```{r, echo=FALSE}
set.seed(2018)

x <- matrix(rnorm(1000*20, mean=0, sd=1) ,1000, 20)
varepsilon <- rnorm(1000, mean=0, sd=1)
y <- sin(2*x[,1]) + 2 + varepsilon
train <- data.frame(y,x)
all.train <- train[order(train[,2]),]

x <- matrix(rnorm(10000*20, mean=0, sd=1) ,10000, 20)
varepsilon <- rnorm(10000, mean=0, sd=1)
y <- sin(2*x[,1]) + 2 + varepsilon
test <- data.frame(y, x)
all.test <- test[order(test[,2]),]
rm(x,y)

kvec <- c(2:15)
kknn.test <- list()
fitted.test <- data.frame(matrix(NA, nrow = 10000, ncol = length(kvec)))
mse.kknn <- data.frame(matrix(NA, nrow = length(kvec), ncol = 20))

for (j in 1:20) {
  train <- all.train[,1:(j+1)]
  test <- all.test[,1:(j+1)]
  for (i in 1:length(kvec)) {
  fitted.test <- kknn(y~., train, test, k = kvec[i], kernel = "rectangular")$fitted 
  mse.kknn[i,j] <- mean((test$y-fitted.test)^2)
  } 
  ls <- lm(train$y~., train)
  mse.ls[j] <- mean((test$y-ls$coefficients[1]-t(as.matrix(ls$coefficients[-1]))%*%t(as.matrix(test[, -1])))^2)
}

colors <- colorRampPalette(c("blue", "red"))(20)
for (j in 1:20) {
par(new=TRUE)
plot(log(1/kvec), mse.kknn[,j], ylim = c(1,2), type = "l", col = colors[j])+ 
abline(h = mse.ls[j], lwd=2, col = colors[j], lty = 3)
}

cat("for large train dataset linear model fits better when No. of variables increase;\n or best k for knn model increase")
```

